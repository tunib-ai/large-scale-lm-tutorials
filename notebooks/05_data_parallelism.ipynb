{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallelism\n",
    "\n",
    "ì´ë²ˆ ì„¸ì…˜ì—ëŠ” ë°ì´í„° ë³‘ë ¬í™” ê¸°ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## 1. `torch.nn.DataParallel`\n",
    "ê°€ì¥ ë¨¼ì € ìš°ë¦¬ì—ê²Œ ì¹œìˆ™í•œ `torch.nn.DataParallel`ì˜ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤. `torch.nn.DataParallel`ì€ single-node & multi-GPUì—ì„œ ë™ì‘í•˜ëŠ” multi-thread ëª¨ë“ˆì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Forward Pass\n",
    "\n",
    "1. ì…ë ¥ëœ mini-batchë¥¼ **Scatter**í•˜ì—¬ ê° ë””ë°”ì´ìŠ¤ë¡œ ì „ì†¡.\n",
    "2. GPU-1ì— ì˜¬ë¼ì™€ ìˆëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ GPU-2,3,4ë¡œ **Broadcast**.\n",
    "3. ê° ë””ë°”ì´ìŠ¤ë¡œ ë³µì œëœ ëª¨ë¸ë¡œ **Forward**í•˜ì—¬ Logitsì„ ê³„ì‚° í•¨.\n",
    "4. ê³„ì‚°ëœ Logitsì„ **Gather**í•˜ì—¬ GPU-1ì— ëª¨ìŒ.\n",
    "5. Logitsìœ¼ë¡œë¶€í„° **Loss**ë¥¼ ê³„ì‚°í•¨. (with loss reduction)\n",
    "\n",
    "![](../images/dp_forward.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "ì½”ë“œë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def data_parallel(module, inputs, labels, device_ids, output_device):\n",
    "    inputs = nn.parallel.scatter(inputs, device_ids)\n",
    "    # ì…ë ¥ ë°ì´í„°ë¥¼ device_idsë“¤ì— Scatterí•¨\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    # ëª¨ë¸ì„ device_idsë“¤ì— ë³µì œí•¨.\n",
    "   \n",
    "    logit = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    # ê° deviceì— ë³µì œëœ ëª¨ë¸ì´ ê° deviceì˜ ë°ì´í„°ë¥¼ Forwardí•¨.\n",
    "\n",
    "    logits = nn.parallel.gather(outputs, output_device)\n",
    "    # ëª¨ë¸ì˜ logitì„ output_device(í•˜ë‚˜ì˜ device)ë¡œ ëª¨ìŒ\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Backward Pass\n",
    "\n",
    "1. ê³„ì‚°ëœ Lossë¥¼ ê° ë””ë°”ì´ìŠ¤ì— **Scatter**í•¨.\n",
    "2. ì „ë‹¬ë°›ì€ Lossë¥¼ ì´ìš©í•´ì„œ ê° ë””ë°”ì´ìŠ¤ì—ì„œ **Backward**ë¥¼ ìˆ˜í–‰í•˜ì—¬ Gradients ê³„ì‚°.\n",
    "3. ê³„ì‚°ëœ ëª¨ë“  Gradientë¥¼ GPU-1ë¡œ **Reduce**í•˜ì—¬ GPU-1ì— ì „ë¶€ ë”í•¨.\n",
    "4. ë”í•´ì§„ Gradientsë¥¼ ì´ìš©í•˜ì—¬ GPU-1ì— ìˆëŠ” ëª¨ë¸ì„ ì—…ë°ì´íŠ¸.\n",
    "\n",
    "![](../images/dp_backward.png)\n",
    "\n",
    "\n",
    "#### í˜¹ì‹œë‚˜ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì„ ìœ„í•´...\n",
    "- `loss.backward()`: ê¸°ìš¸ê¸°ë¥¼ ë¯¸ë¶„í•´ì„œ Gradientë¥¼ ê³„ì‚°\n",
    "- `optimizer.step()`: ê³„ì‚°ëœ Gradientë¥¼ ì´ìš©í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸\n",
    "- Computation costëŠ” `backward()` > `step()`.\n",
    "\n",
    "![](../images/backward_step.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. create dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "data_loader = DataLoader(datasets, batch_size=128, num_workers=4)\n",
    "\n",
    "# 2. create model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "\n",
    "# 3. make data parallel module\n",
    "# device_ids: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ / output_device: ì¶œë ¥ê°’ì„ ëª¨ì„ ë””ë°”ì´ìŠ¤\n",
    "model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)\n",
    "\n",
    "# 4. create optimizer and loss fn\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# 5. start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    loss = loss_fn(logits, data[\"labels\"].cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 58.31it/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "step:0, loss:1.1612184047698975\n",
      "step:10, loss:1.1026676893234253\n",
      "step:20, loss:1.0577733516693115\n",
      "step:30, loss:0.9685771465301514\n",
      "step:40, loss:0.8478926420211792\n",
      "step:50, loss:0.8693557977676392\n",
      "step:60, loss:0.7827763557434082\n",
      "step:70, loss:0.7895966172218323\n",
      "step:80, loss:0.7631332278251648\n",
      "step:90, loss:0.6766361594200134\n",
      "step:100, loss:0.6931278109550476\n",
      "step:110, loss:0.7477961778640747\n",
      "step:120, loss:0.7386300563812256\n",
      "step:130, loss:0.7414667010307312\n",
      "step:140, loss:0.7170238494873047\n",
      "step:150, loss:0.7286601066589355\n",
      "step:160, loss:0.7063153982162476\n",
      "step:170, loss:0.6415464282035828\n",
      "step:180, loss:0.7068504095077515\n",
      "step:190, loss:0.593433678150177\n",
      "step:200, loss:0.6224725246429443\n",
      "step:210, loss:0.7025654315948486\n",
      "step:220, loss:0.5605336427688599\n",
      "step:230, loss:0.578403890132904\n",
      "step:240, loss:0.7344318628311157\n",
      "step:250, loss:0.5977576971054077\n",
      "step:260, loss:0.6717301607131958\n",
      "step:270, loss:0.7103744745254517\n",
      "step:280, loss:0.6679482460021973\n",
      "step:290, loss:0.635512113571167\n",
      "step:300, loss:0.45178914070129395\n"
     ]
    }
   ],
   "source": [
    "!python ../src/data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/dp_training.png)\n",
    "\n",
    "Multi-GPUì—ì„œ í•™ìŠµì´ ì˜ ë˜ëŠ”êµ°ìš”. ê·¸ëŸ°ë° ë¬¸ì œëŠ” 0ë²ˆ GPUì— Logitsì´ ì ë¦¬ë‹¤ë³´ë‹ˆ GPU ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œê°€ ì¼ì–´ë‚©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” 0ë²ˆ deviceë¡œ Logitsì´ ì•„ë‹Œ Lossë¥¼ Gatherí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ë©´ ì–´ëŠì •ë„ ì™„í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Logitsì— ë¹„í•´ LossëŠ” Scalarì´ê¸° ë•Œë¬¸ì— í¬ê¸°ê°€ í›¨ì”¬ ì‘ê¸° ë•Œë¬¸ì´ì£ . ì´ ì‘ì—…ì€ [ë‹¹ê·¼ë§ˆì¼“ ë¸”ë¡œê·¸](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)ì— ì†Œê°œë˜ì—ˆë˜ [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)ì˜ `DataParallelCriterion`ê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ì— ê½¤ë‚˜ ë³µì¡í•˜ê²Œ ì„¤ëª…ë˜ì–´ ìˆëŠ”ë°, ë³µì¡í•œ ë°©ë²• ëŒ€ì‹  ê°„ë‹¨í•˜ê²Œ **forward í•¨ìˆ˜ë¥¼ ì˜¤ë²„ë¼ì´ë“œ í•˜ëŠ” ê²ƒ** ë§Œìœ¼ë¡œ ë™ì¼ ê¸°ëŠ¥ì„ ì‰½ê²Œ êµ¬í˜„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/dp_forward_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "í•µì‹¬ì€ Loss Computationê³¼ Lossê°€ reductionì„ multi-thread ì•ˆì—ì„œ ì‘ë™ ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì˜ forward í•¨ìˆ˜ëŠ” multi-threadì—ì„œ ì‘ë™ë˜ê³  ìˆê¸° ë•Œë¬¸ì— Loss Computation ë¶€ë¶„ì„ forward í•¨ìˆ˜ ì•ˆì— ë„£ìœ¼ë©´ ë§¤ìš° ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê² ì£ .\n",
    "\n",
    "í•œê°€ì§€ íŠ¹ì´í•œ ì ì€ ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´ Lossì˜ reductionì´ 2ë²ˆ ì¼ì–´ë‚˜ê²Œ ë˜ëŠ”ë°ìš”. multi-threadì—ì„œ batch_size//4ê°œì—ì„œ 4ê°œë¡œ reduction ë˜ëŠ” ê³¼ì •(ê·¸ë¦¼ì—ì„œ 4ë²ˆ)ì´ í•œë²ˆ ì¼ì–´ë‚˜ê³ , ê° ë””ë°”ì´ìŠ¤ì—ì„œ ì¶œë ¥ëœ 4ê°œì˜ Lossë¥¼ 1ê°œë¡œ Reduction í•˜ëŠ” ê³¼ì •(ê·¸ë¦¼ì—ì„œ 5ë²ˆ)ì´ ë‹¤ì‹œ ì¼ì–´ë‚˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ê³  í•˜ë”ë¼ë„ Loss computation ë¶€ë¶„ì„ ë³‘ë ¬í™” ì‹œí‚¬ ìˆ˜ ìˆê³ , 0ë²ˆ GPUì— ê°€í•´ì§€ëŠ” ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ê¸° ë•Œë¬¸ì— í›¨ì”¬ íš¨ìœ¨ì ì´ì£ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/custom_data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# logitsì„ ì¶œë ¥í•˜ëŠ” ì¼ë°˜ì ì¸ ëª¨ë¸\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.linear(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# forward passì—ì„œ lossë¥¼ ì¶œë ¥í•˜ëŠ” parallel ëª¨ë¸\n",
    "class ParallelLossModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        logits = super(ParallelLossModel, self).forward(inputs)\n",
    "        loss = nn.CrossEntropyLoss(reduction=\"mean\")(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš´ì´ ì¢‹ê²Œë„ ìš°ë¦¬ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” Huggingface Transformers ëª¨ë¸ë“¤ì€ forward passì—ì„œ ê³§ ë°”ë¡œ Lossë¥¼ êµ¬í•˜ëŠ” ê¸°ëŠ¥ì„ ë‚´ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ ê³¼ì • ì—†ì´ transformersì˜ ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œëŠ” Transformers ëª¨ë¸ì˜ `labels`ì¸ìì— ë¼ë²¨ì„ ì…ë ¥í•˜ì—¬ Lossë¥¼ ë°”ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/efficient_data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "# 1 ~ 4ê¹Œì§€ ìƒëµ...\n",
    "\n",
    "# 5. start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "    \n",
    "    loss = loss.mean()\n",
    "    # (4,) -> (1,)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 199.34it/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/kevin/kevin_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "step:0, loss:1.186471700668335\n",
      "step:10, loss:1.1163532733917236\n",
      "step:20, loss:1.091385841369629\n",
      "step:30, loss:1.0980195999145508\n",
      "step:40, loss:1.0779412984848022\n",
      "step:50, loss:1.053116798400879\n",
      "step:60, loss:0.9878815412521362\n",
      "step:70, loss:0.9763977527618408\n",
      "step:80, loss:0.8458528518676758\n",
      "step:90, loss:0.8098542094230652\n",
      "step:100, loss:0.7924742698669434\n",
      "step:110, loss:0.8259536027908325\n",
      "step:120, loss:0.8083906173706055\n",
      "step:130, loss:0.7789419889450073\n",
      "step:140, loss:0.7848180532455444\n",
      "step:150, loss:0.7716841697692871\n",
      "step:160, loss:0.7316021919250488\n",
      "step:170, loss:0.6465802192687988\n",
      "step:180, loss:0.7471408843994141\n",
      "step:190, loss:0.5954912900924683\n",
      "step:200, loss:0.6941753029823303\n",
      "step:210, loss:0.7786209583282471\n",
      "step:220, loss:0.6332131028175354\n",
      "step:230, loss:0.6579948663711548\n",
      "step:240, loss:0.7271711230278015\n",
      "step:250, loss:0.5837332010269165\n",
      "step:260, loss:0.6737046241760254\n",
      "step:270, loss:0.6502429246902466\n",
      "step:280, loss:0.6647026538848877\n",
      "step:290, loss:0.6707975268363953\n",
      "step:300, loss:0.47382402420043945\n"
     ]
    }
   ],
   "source": [
    "!python ../src/efficient_data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## 2. `torch.nn.DataParallel`ì˜ ë¬¸ì œì \n",
    "\n",
    "\n",
    "### 1) ë©€í‹°ì“°ë ˆë“œ ëª¨ë“ˆì´ê¸° ë•Œë¬¸ì— Pythonì—ì„œ ë¹„íš¨ìœ¨ì ì„.\n",
    "Pythonì€ GIL (Global Interpreter Lock)ì— ì˜í•´ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì—¬ëŸ¬ê°œì˜ ì“°ë ˆë“œê°€ ì‘ë™ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê·¼ë³¸ì ìœ¼ë¡œ ë©€í‹° ì“°ë ˆë“œê°€ ì•„ë‹Œ **ë©€í‹° í”„ë¡œì„¸ìŠ¤ í”„ë¡œê·¸ë¨**ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì—¬ëŸ¬ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ê²Œ í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ì—…ë°ì´íŠ¸ ëœ ëª¨ë¸ì´ ë‹¤ë¥¸ deviceë¡œ ë§¤ ìŠ¤í…ë§ˆë‹¤ ë³µì œë˜ì–´ì•¼ í•¨.\n",
    "í˜„ì¬ì˜ ë°©ì‹ì€ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ê³„ì‚°ëœ Gradientë¥¼ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì„œ(Gather) ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ì„ ë§¤ë²ˆ ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ë“¤ë¡œ ë³µì œ(Broadcast)í•´ì•¼ í•˜ëŠ”ë°, ì´ ê³¼ì •ì´ ê½¤ë‚˜ ë¹„ìŒ‰ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Gradientë¥¼ Gatherí•˜ì§€ ì•Šê³  ê° ë””ë°”ì´ìŠ¤ì—ì„œ ìì²´ì ìœ¼ë¡œ `step()`ì„ ìˆ˜í–‰í•œë‹¤ë©´ ëª¨ë¸ì„ ë§¤ë²ˆ ë³µì œí•˜ì§€ ì•Šì•„ë„ ë˜ê² ì£ . ì–´ë–»ê²Œ ì´ ê²ƒì„ êµ¬í˜„ í•  ìˆ˜ ìˆì„ê¹Œìš”?\n",
    "\n",
    "<br>\n",
    "\n",
    "### Solution? â All-reduce!! ğŸ‘\n",
    "![](../images/allreduce.png)\n",
    "\n",
    "ì •ë‹µì€ ì•ì„œ ë°°ì› ë˜ All-reduce ì—°ì‚°ì…ë‹ˆë‹¤. ê° ë””ë°”ì´ìŠ¤ì—ì„œ ê³„ì‚°ëœ Gradientsë¥¼ ëª¨ë‘ ë”í•´ì„œ ëª¨ë“  ë””ë°”ì´ìŠ¤ì— ê· ì¼í•˜ê²Œ ë¿Œë ¤ì¤€ë‹¤ë©´ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ìì²´ì ìœ¼ë¡œ `step()`ì„ ìˆ˜í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ë§¤ë²ˆ ëª¨ë¸ì„ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œë¶€í„° ë³µì œí•´ ì˜¬ í•„ìš”ê°€ ì—†ê² ì£ . ë”°ë¼ì„œ All-reduceë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ê·¸ëŸ¬ë‚˜...  ğŸ¤”\n",
    "ê·¸ëŸ¬ë‚˜ All-reduceëŠ” ë§¤ìš° ë¹„ìš©ì´ ë†’ì€ ì—°ì‚°ì— ì†í•©ë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”? All-reduceì˜ ì„¸ë¶€ êµ¬í˜„ì„ ì‚´í´ë´…ì‹œë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Reduce + Broadcast êµ¬í˜„ ë°©ì‹\n",
    "![](../images/allreduce_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### All to All êµ¬í˜„ ë°©ì‹\n",
    "![](../images/allreduce_2.png)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `torch.nn.parallel.DistributedDataParallel` (ì´í•˜ DDP)\n",
    "\n",
    "### Ring All-reduce ğŸ’\n",
    "Ring All-reduceëŠ” 2017ë…„ì— ë°”ì´ë‘ì˜ ì—°êµ¬ì§„ì´ ê°œë°œí•œ ìƒˆë¡œìš´ ì—°ì‚°ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì‹ë“¤ì— ë¹„í•´ ì›”ë“±íˆ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— DDP ê°œë°œì˜ í•µì‹¬ì´ ë˜ì—ˆì£ .\n",
    "\n",
    "- https://github.com/baidu-research/baidu-allreduce\n",
    "\n",
    "![](../images/ring_allreduce.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/ring_allreduce.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### DDPë€?\n",
    "DDPëŠ” ê¸°ì¡´ DataParallelì˜ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ë°ì´í„° ë³‘ë ¬ì²˜ë¦¬ ëª¨ë“ˆì´ë©° single/multi-node & multi-GPUì—ì„œ ë™ì‘í•˜ëŠ” multi-process ëª¨ë“ˆì…ë‹ˆë‹¤. All-reduceë¥¼ í™œìš©í•˜ê²Œ ë˜ë©´ì„œ ë§ˆìŠ¤í„° í”„ë¡œì„¸ìŠ¤ì˜ ê°œë…ì´ ì—†ì–´ì¡Œê¸° ë•Œë¬¸ì— í•™ìŠµ ê³¼ì •ì´ ë§¤ìš° ì‹¬í”Œí•˜ê²Œ ë³€í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/ddp.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ddp.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. initialize process group\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "world_size = dist.get_world_size()\n",
    "torch.cuda.set_device(rank)\n",
    "device = torch.cuda.current_device()\n",
    "\n",
    "# 2. create dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "\n",
    "# 3. create DistributedSampler\n",
    "# DistributedSamplerëŠ” ë°ì´í„°ë¥¼ ìª¼ê°œì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ì†¡í•˜ê¸° ìœ„í•œ ëª¨ë“ˆì…ë‹ˆë‹¤.\n",
    "sampler = DistributedSampler(\n",
    "    datasets,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    datasets,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    sampler=sampler,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# 4. create model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "# 5. make distributed data parallel module\n",
    "model = DistributedDataParallel(model, device_ids=[device], output_device=device)\n",
    "\n",
    "# 5. create optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# 6. start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0 and rank == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ê¸° ë•Œë¬¸ì— `torch.distributed.launch`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Using custom data configuration default\n",
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 181.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 149.46it/s]\n",
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 229.28it/s]\n",
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 361.84it/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "step:0, loss:1.1451387405395508\n",
      "step:10, loss:1.0912988185882568\n",
      "step:20, loss:1.0485237836837769\n",
      "step:30, loss:0.9971571564674377\n",
      "step:40, loss:0.9472718238830566\n",
      "step:50, loss:1.0532103776931763\n",
      "step:60, loss:0.6478840112686157\n",
      "step:70, loss:0.9035330414772034\n",
      "step:80, loss:0.8176743388175964\n",
      "step:90, loss:1.058182716369629\n",
      "step:100, loss:0.7739772796630859\n",
      "step:110, loss:0.6652507185935974\n",
      "step:120, loss:0.7778272032737732\n",
      "step:130, loss:0.827933669090271\n",
      "step:140, loss:0.6303764581680298\n",
      "step:150, loss:0.5062040090560913\n",
      "step:160, loss:0.8570529222488403\n",
      "step:170, loss:0.6550942063331604\n",
      "step:180, loss:0.6157522797584534\n",
      "step:190, loss:0.7612558007240295\n",
      "step:200, loss:0.7380551099777222\n",
      "step:210, loss:0.7818665504455566\n",
      "step:220, loss:0.9607051610946655\n",
      "step:230, loss:0.8241059184074402\n",
      "step:240, loss:0.5454672574996948\n",
      "step:250, loss:0.4731343686580658\n",
      "step:260, loss:0.8883727788925171\n",
      "step:270, loss:0.4605785310268402\n",
      "step:280, loss:0.7553415298461914\n",
      "step:290, loss:0.8398311138153076\n",
      "step:300, loss:0.45668572187423706\n"
     ]
    }
   ],
   "source": [
    "!python -m  torch.distributed.launch --nproc_per_node=4 ../src/ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ëŸ°ë° ì ê¹, All-reduceë¥¼ ì–¸ì œ ìˆ˜í–‰í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?\n",
    "- All-reduceë¥¼ `backward()`ì—°ì‚°ê³¼ í•¨ê»˜ í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?\n",
    "- ì•„ë‹ˆë©´ `backward()`ê°€ ëª¨ë‘ ëë‚˜ê³  `step()` ì‹œì‘ ì „ì— í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?\n",
    "\n",
    "![](../images/ddp_analysis_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### ê²°ê³¼ì ìœ¼ë¡œ `backward()`ì™€ `all-reduce`ë¥¼ ì¤‘ì²©ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°ê³¼ì ìœ¼ë¡œ `backward()`ì™€ `all-reduce`ë¥¼ ì¤‘ì²©ì‹œí‚¤ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ì‹ì…ë‹ˆë‹¤. `all_reduce`ëŠ” ë„¤íŠ¸ì›Œí¬ í†µì‹ , `backward()`, `step()` ë“±ì€ GPU ì—°ì‚°ì´ê¸° ë•Œë¬¸ì— ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆì£ . ì´ë“¤ì„ ì¤‘ì²©ì‹œí‚¤ë©´ ì¦‰, computationê³¼ communicationì´ ìµœëŒ€í•œìœ¼ë¡œ overlap ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° íš¨ìœ¨ì´ í¬ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/ddp_analysis_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "ë¶„ì„ ê²°ê³¼ `backward()`ì™€ `step()`ì„ ë¹„êµí•´ë³´ë©´ `backward()`ê°€ í›¨ì”¬ ë¬´ê±°ìš´ ì—°ì‚°ì´ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/ddp_analysis_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "ë‹¹ì—°íˆ ë” ë¬´ê±°ìš´ ì—°ì‚°ì„ ì¤‘ì²©ì‹œí‚¬ ìˆ˜ë¡ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œê°„ì´ ì§§ì•„ì§‘ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ `backward()`ê°€ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒ ë³´ë‹¤ `all-reduce`ë¥¼ í•¨ê»˜ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë¹¨ëìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/ddp_analysis_4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### ì´ ë•Œ, ìƒê¸¸ ìˆ˜ ìˆëŠ” ê¶ê¸ˆì¦ë“¤...\n",
    "- Q1: `backward()` ì—°ì‚° ì¤‘ì— Gradientê°€ ëª¨ë‘ ê³„ì‚°ë˜ì§€ ì•Šì•˜ëŠ”ë° ì–´ë–»ê²Œ `all-reduce`ë¥¼ ìˆ˜í–‰í•©ë‹ˆê¹Œ?\n",
    "  - A1: `backward()`ëŠ” ë’¤ìª½ ë ˆì´ì–´ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ê³„ì‚°ì´ ëë‚œ ë ˆì´ì–´ ë¨¼ì € ì „ì†¡í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Q2: ê·¸ë ‡ë‹¤ë©´ ì–¸ì œë§ˆë‹¤ `all-reduce`ë¥¼ ìˆ˜í–‰í•˜ë‚˜ìš”? ë ˆì´ì–´ë§ˆë‹¤ ì´ë£¨ì–´ì§€ë‚˜ìš”?\n",
    "  - A2: ì•„ë‹™ë‹ˆë‹¤. Gradient Bucketingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Bucketì´ ê°€ë“ì°¨ë©´ All-reduceë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Gradient Bucekting\n",
    "Gradient BucektingëŠ” Gradientë¥¼ ì¼ì •í•œ ì‚¬ì´ì¦ˆì˜ bucketì— ì €ì¥í•´ë‘ê³  ê°€ë“ì°¨ë©´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ì†¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € `backward()` ì—°ì‚° ë„ì¤‘ ë’¤ìª½ë¶€í„° ê³„ì‚°ëœ Gradientë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ bucketì— ì €ì¥í•˜ë‹¤ê°€ bucketì˜ ìš©ëŸ‰ì´ ê°€ë“ì°¨ë©´ All-reduceë¥¼ ìˆ˜í–‰í•´ì„œ ê° deviceì— Gradientì˜ í•©ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ê·¸ë¦¼ ë•Œë¬¸ì— í—·ê°ˆë¦´ ìˆ˜ë„ ìˆëŠ”ë°, bucketì— ì €ì¥ë˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ í•´ë‹¹ ë ˆì´ì–´ì—ì„œ ì¶œë ¥ëœ Gradientì…ë‹ˆë‹¤. ëª¨ë“  bucketì€ ì¼ì •í•œ ì‚¬ì´ì¦ˆë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° `bucket_size_mb` ì¸ìë¥¼ í†µí•´ mega-byte ë‹¨ìœ„ë¡œ ìš©ëŸ‰ì„ ì„¤ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/ddp_analysis_5.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
